{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0k5aaY3X5WFK"},"outputs":[],"source":["!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6M8q_xkh4_Du"},"outputs":[],"source":["import dgl\n","import dgl.nn as dgl_nn\n","import dgl.function as dgl_f \n","import torch \n","import torch.nn as nn\n","import torch.nn.functional as F\n","import networkx as nx \n","from metra import *\n","from utils import *\n","from tqdm.notebook import tqdm\n","from IPython.display import clear_output\n","import scipy.sparse as sparse\n","import math\n","\n","import os\n","import ssl\n","from six.moves import urllib\n","import torch\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","\n","torch.manual_seed(123)"]},{"cell_type":"markdown","metadata":{"id":"5gduSM83Eb4O"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gag0dvcs8IrE"},"outputs":[],"source":["def download_file(dataset):\n","    print(\"Start Downloading data: {}\".format(dataset))\n","    url = \"https://s3.us-west-2.amazonaws.com/dgl-data/dataset/{}\".format(\n","        dataset)\n","    print(\"Start Downloading File....\")\n","    context = ssl._create_unverified_context()\n","    data = urllib.request.urlopen(url, context=context)\n","    with open(\"./data/{}\".format(dataset), \"wb\") as handle:\n","        handle.write(data.read())\n","\n","\n","class SnapShotDataset(Dataset):\n","    def __init__(self, path, npz_file):\n","        if not os.path.exists(path+'/'+npz_file):\n","            if not os.path.exists(path):\n","                os.mkdir(path)\n","            download_file(npz_file)\n","        zipfile = np.load(path+'/'+npz_file)\n","        self.x = zipfile['x']\n","        self.y = zipfile['y']\n","\n","    def __len__(self):\n","        return len(self.x)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        return self.x[idx, ...], self.y[idx, ...]\n","\n","\n","def METR_LAGraphDataset():\n","    if not os.path.exists('data/graph_la.bin'):\n","        if not os.path.exists('data'):\n","            os.mkdir('data')\n","        download_file('graph_la.bin')\n","    g, _ = dgl.load_graphs('data/graph_la.bin')\n","    return g[0]\n","\n","\n","class METR_LATrainDataset(SnapShotDataset):\n","    def __init__(self):\n","        super(METR_LATrainDataset, self).__init__('data', 'metr_la_train.npz')\n","        #self.mean = self.x[..., 0].mean()\n","        #self.std = self.x[..., 0].std()\n","        print(self.x.shape)\n","        self.mean = np.mean(self.x, axis = (0,1,2))\n","        self.std = np.std(self.x, axis = (0,1,2))\n","\n","\n","class METR_LATestDataset(SnapShotDataset):\n","    def __init__(self):\n","        super(METR_LATestDataset, self).__init__('data', 'metr_la_test.npz')\n","\n","\n","class METR_LAValidDataset(SnapShotDataset):\n","    def __init__(self):\n","        super(METR_LAValidDataset, self).__init__('data', 'metr_la_valid.npz')\n","\n","\n","def PEMS_BAYGraphDataset():\n","    if not os.path.exists('data/graph_bay.bin'):\n","        if not os.path.exists('data'):\n","            os.mkdir('data')\n","        download_file('graph_bay.bin')\n","    g, _ = dgl.load_graphs('data/graph_bay.bin')\n","    return g[0]\n","\n","\n","class PEMS_BAYTrainDataset(SnapShotDataset):\n","    def __init__(self):\n","        super(PEMS_BAYTrainDataset, self).__init__(\n","            'data', 'pems_bay_train.npz')\n","        self.mean = np.mean(self.x, axis = (0,1,2))\n","        self.std = np.std(self.x, axis = (0,1,2))\n","\n","\n","class PEMS_BAYTestDataset(SnapShotDataset):\n","    def __init__(self):\n","        super(PEMS_BAYTestDataset, self).__init__('data', 'pems_bay_test.npz')\n","\n","\n","class PEMS_BAYValidDataset(SnapShotDataset):\n","    def __init__(self):\n","        super(PEMS_BAYValidDataset, self).__init__(\n","            'data', 'pems_bay_valid.npz')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2w2aRzV1RhY"},"outputs":[],"source":["def BufflogroveGraphDataset():\n","    g, _ = dgl.load_graphs('data/Buffalogrove/graph_buffalo.bin')\n","    return g[0]\n","\n","class BuffalogroveTrainDataset(SnapShotDataset):\n","    def __init__(self, num_steps = 3):\n","        super(BuffalogroveTrainDataset, self).__init__(\"data/Buffalogrove\", f\"buffalogrove_train_data_{num_steps}_steps.npz\")\n","        print(self.x.shape, self.y.shape)\n","        self.mean = np.mean(self.x, axis = (0,1,2))\n","        self.std = np.std(self.x, axis = (0,1,2))\n","        \n","class BuffalogroveValDataset(SnapShotDataset):\n","    def __init__(self, num_steps = 3):\n","        super(BuffalogroveValDataset, self).__init__(\"data/Buffalogrove\", f\"buffalogrove_val_data_{num_steps}_steps.npz\")\n","\n","class BuffalogroveTestDataset(SnapShotDataset):\n","    def __init__(self, date, num_steps = 3):\n","        filename = \"buffalogrove_test_\" + date + f\"_data_{num_steps}_steps.npz\"\n","        super(BuffalogroveTestDataset, self).__init__(\"data/Buffalogrove\", filename)\n","\n","def GurneeGraphDataset():\n","    g, _ = dgl.load_graphs('data/Gurnee/graph_gurnee.bin')\n","    return g[0]\n","\n","class GurneeTrainDataset(SnapShotDataset):\n","    def __init__(self, num_steps = 3):\n","        super(GurneeTrainDataset, self).__init__(\"data/Gurnee\", f\"gurnee_train_data_{num_steps}_steps.npz\")\n","        print(self.x.shape)\n","        self.mean = np.mean(self.x, axis = (0,1,2))\n","        self.std = np.std(self.x, axis = (0,1,2))\n","        \n","class GurneeValDataset(SnapShotDataset):\n","    def __init__(self, num_steps = 3):\n","        super(GurneeValDataset, self).__init__(\"data/Gurnee\", f\"gurnee_val_data_{num_steps}_steps.npz\")\n","\n","class GurneeTestDataset(SnapShotDataset):\n","    def __init__(self, date, num_steps =3 ):\n","        filename = \"gurnee_test_\" + date + f\"_data_{num_steps}_steps.npz\"\n","        super(GurneeTestDataset, self).__init__(\"data/Gurnee\", filename)"]},{"cell_type":"markdown","metadata":{"id":"1DOOUogfEn1F"},"source":["## Model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RyFlHSgY5M_c"},"outputs":[],"source":["from typing import Optional\n","\n","import torch\n","import torch.nn as nn\n","class STGPCN(nn.Module):\n","    r\"\"\"\n","    Args:\n","        in_channels (int): Number of input features.\n","        n_filter (int): Number of filters.\n","        time_strides (int): Time strides during temporal convolution.\n","        num_for_predict (int): Number of predictions to make in the future.\n","        len_input (int): Length of the input sequence.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_channels: int,\n","        n_filter: int,\n","        time_strides: int,\n","        num_for_predict: int,\n","        len_input: int,\n","        bias: bool = True,\n","    ):\n","\n","        super(D3TGCN3, self).__init__()\n","        print('Start')\n","\n","        self._conv1 = nn.Conv3d(in_channels=1, out_channels=256, kernel_size=(1, 1, len_input), groups=1)\n","        self._pool1 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self._conv2 = nn.Conv3d(in_channels=1, out_channels=128, kernel_size=(1, 1, 128), groups=1)\n","        self._pool2 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self.layer_norm = nn.LayerNorm([64])\n","        self._conv3 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=(1, 1, 64), groups=1)\n","        self._pool3 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self._conv4 = nn.Conv3d(in_channels=1, out_channels=len_input, kernel_size=(1, 1, 32), groups=1)\n","\n","        self._conv1_1 = nn.Conv3d(in_channels=2, out_channels=256, kernel_size=(1, 1, len_input), groups=1)\n","        self._pool1_1 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self._conv2_1 = nn.Conv3d(in_channels=1, out_channels=128, kernel_size=(1, 1, 128), groups=1)\n","        self._pool2_1 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self.layer_norm_1 = nn.LayerNorm([64])\n","        self._conv3_1 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=(1, 1, 64), groups=1)\n","        self._pool3_1 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self._conv4_1 = nn.Conv3d(in_channels=1, out_channels=len_input, kernel_size=(1, 1, 32), groups=1)\n","\n","        self._conv1_2 = nn.Conv3d(in_channels=2, out_channels=256, kernel_size=(1, 1, len_input), groups=1)\n","        self._pool1_2 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self._conv2_2 = nn.Conv3d(in_channels=1, out_channels=128, kernel_size=(1, 1, 128), groups=1)\n","        self._pool2_2 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self.layer_norm_2 = nn.LayerNorm([64])\n","        self._conv3_2 = nn.Conv3d(in_channels=1, out_channels=64, kernel_size=(1, 1, 64), groups=1)\n","        self._pool3_2 = nn.MaxPool3d((1, 1, 2), stride=(1, 1, 2))\n","        self._conv4_2 = nn.Conv3d(in_channels=1, out_channels=len_input, kernel_size=(1, 1, 32), groups=1)\n","\n","        self._final_conv = nn.Conv3d(in_channels=2, out_channels=num_for_predict, kernel_size=(1, 1, len_input), groups=1)\n","\n","        self._reset_parameters()\n","\n","    def _reset_parameters(self):\n","        \"\"\"\n","        Resetting the parameters.\n","        \"\"\"\n","        for p in self.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","            else:\n","                nn.init.uniform_(p)\n","\n","    def forward(self, X: torch.FloatTensor) -> torch.FloatTensor:\n","        \"\"\"\n","        Making a forward pass.\n","\n","        Arg types:\n","            * **X** (PyTorch FloatTensor) - Node features for T time periods, with shape (B, N_nodes, F_in, T_in).\n","           \n","        Return types:\n","            * **X** (PyTorch FloatTensor)* - Hidden state tensor for all nodes, with shape (B, N_nodes, T_out).\n","        \"\"\"\n","        # X = (B, N, I, T) (16, 318, 1, 6)  \n","        X = torch.unsqueeze(X, 1)  # (B, 1, NxE, I, T)  (16, 1, 318, 1, 6)\n","        Z = X.clone()  \n","        X = self._conv1(X)  # => (16, 256, 318, 1, 1) \n","        # X = torch.squeeze(X, 4)  # => (16, 6, 318, 1)  \n","        # X = torch.unsqueeze(X, 4)\n","        # X = X.permute(0, 4, 1, 2, 3)  # => (16, 1, 256, 318, 1) \n","        X = X.permute(0, 4, 2, 3, 1)  # => (16, 1, 318, 1, 256) \n","        # X = X.permute(0, 2, 1, 3)\n","        # (b,N,F,T)->(b,T,N,F)-conv<1,F>->(b,c_out*T,N,1) \n","        # for example (32, 307, 64, 12) -permute-> (32, 12, 307,64) -final_conv-> (32, 12, 307, 1)\n","        # X = self._final_conv(X.permute(0, 3, 1, 2))\n","        X = self._pool1(X)\n","        X = self._conv2(X)  # => (16, 1, 318, 1, 128)\n","        X = X.permute(0, 4, 2, 3, 1) \n","        # X = self._conv3(X)  # => (16, 1, 318, 1, 64) \n","        X = self._pool2(X) \n","        X = self.layer_norm(X)\n","        X = self._conv3(X)  # => (16, 1, 318, 1, 128) \n","        X = X.permute(0, 4, 2, 3, 1) \n","        # # X = self._conv3(X)  # => (16, 1, 318, 1, 64) \n","        X = self._pool3(X) \n","        X = self._conv4(X)  # => (16, 1, 318, 1, 128) \n","        X = X.permute(0, 4, 2, 3, 1) \n","        Z_ = X.clone()  \n","        X = torch.cat((X, Z), 1)\n","\n","        # Second block\n","        X = self._conv1_1(X)  # => (16, 256, 318, 1, 1) \n","        X = X.permute(0, 4, 2, 3, 1)  # => (16, 1, 318, 1, 256) \n","        # (b,N,F,T)->(b,T,N,F)-conv<1,F>->(b,c_out*T,N,1) \n","        # X = self._final_conv(X.permute(0, 3, 1, 2))\n","        X = self._pool1_1(X)\n","        X = self._conv2_1(X)  # => (16, 1, 318, 1, 128)\n","        X = X.permute(0, 4, 2, 3, 1) \n","        X = self._pool2_1(X) \n","        X = self.layer_norm(X)\n","        X = self._conv3_1(X)  # => (16, 1, 318, 1, 128) \n","        X = X.permute(0, 4, 2, 3, 1) \n","        X = self._pool3_1(X) \n","        X = self._conv4_1(X)  # => (16, 1, 318, 1, 128) \n","        X = X.permute(0, 4, 2, 3, 1) \n","        X = torch.cat((X, Z_), 1)\n","\n","        # Third block\n","        X = self._conv1_2(X)  # => (16, 256, 318, 1, 1) \n","        X = X.permute(0, 4, 2, 3, 1)  # => (16, 1, 318, 1, 256) \n","        # (b,N,F,T)->(b,T,N,F)-conv<1,F>->(b,c_out*T,N,1) \n","        # X = self._final_conv(X.permute(0, 3, 1, 2))\n","        X = self._pool1_2(X)\n","        X = self._conv2_2(X)  # => (16, 1, 318, 1, 128)\n","        X = X.permute(0, 4, 2, 3, 1) \n","        X = self._pool2_2(X) \n","        X = self.layer_norm(X)\n","        X = self._conv3_2(X)  # => (16, 1, 318, 1, 128) \n","        X = X.permute(0, 4, 2, 3, 1) \n","        X = self._pool3_2(X) \n","        X = self._conv4_2(X)  # => (16, 1, 318, 1, 128) \n","        X = X.permute(0, 4, 2, 3, 1) \n","        X = torch.cat((X, Z_), 1)\n","\n","        X = self._final_conv(X) # => (16, 6, 318, 1, 1) \n","        # X = torch.squeeze(X, 2)  # => (16, 6, 318, 1)  \n","        X = torch.squeeze(X, 4)  # => (16, 6, 318, 1)  \n","        X = X.permute(0, 2, 3, 1)   # => (16, 318, 1, 6)  \n","        # X = torch.squeeze(X, 4)\n","        # print(X.shape)\n","        # (b,c_out*T,N)->(b,N,T)\n","        # X = X[:, :, :, -1] # (b,c_out*T,N) for example (32, 12, 307)\n","        # print(X.shape)\n","        # X = X.permute(0, 2, 1) # (b,T,N)-> (b,N,T)\n","        # print(X.shape)\n","        return X #(b,N,T) for exmaple (32, 307,12)"]},{"cell_type":"markdown","metadata":{"id":"daPjV9eoFTav"},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yl8zXtJvclaM"},"outputs":[],"source":["def calculate_lap_pos(g, k):\n","    A = sparse.coo_matrix.todense(g.adj(scipy_fmt = 'coo'))\n","    A = np.array(A)\n","    L = calculate_normalized_laplacian(A)\n","    lap_pos = laplacian_positional_encoding(L, k)\n","    lap_pos = np.real(lap_pos)\n","    return lap_pos"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21273,"status":"ok","timestamp":1661869740574,"user":{"displayName":"phú nguyễn","userId":"01399564067477344093"},"user_tz":-420},"id":"0V_8ohIv4_Dw","outputId":"314a3a1e-e7c8-4fee-8dac-33309d9a6320"},"outputs":[{"output_type":"stream","name":"stdout","text":["(23974, 12, 207, 2)\n","Mean:  [54.4059283   0.49721458]\n","Std:  [19.49373927  0.28892871]\n","Num node 207\n","Num edge:  1722\n"]}],"source":["scaler = ZScaler()\n","g = METR_LAGraphDataset()\n","train_data = METR_LATrainDataset()\n","test_data = METR_LATestDataset()\n","valid_data = METR_LAValidDataset()\n","\n","# g = PEMS_BAYGraphDataset()\n","# train_data = PEMS_BAYTrainDataset()\n","# test_data = PEMS_BAYTestDataset()\n","# valid_data = PEMS_BAYValidDataset()\n","\n","# g = BufflogroveGraphDataset()\n","# train_data = BuffalogroveTrainDataset(num_steps = 3)\n","# # test_data = BuffalogroveTestDataset()\n","# valid_data = BuffalogroveValDataset(num_steps = 3)\n","\n","# g = GurneeGraphDataset()\n","# train_data = GurneeTrainDataset(num_steps = 6)\n","# test_data = GurneeTestDataset()\n","# valid_data = GurneeValDataset(num_steps = 6)\n","\n","num_node = g.num_nodes()\n","num_edges = g.num_edges()\n","mean = train_data.mean\n","std = train_data.std\n","\n","print(\"Mean: \", mean)\n","print(\"Std: \", std)\n","print(\"Num node\", num_node)\n","print(\"Num edge: \", num_edges)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1661869740575,"user":{"displayName":"phú nguyễn","userId":"01399564067477344093"},"user_tz":-420},"id":"FWUzeh8FDwZ-","outputId":"f3e772c9-f7c9-4af9-a8e3-47232efaa3ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean:  [54.4059283   0.49721458]\n","Std:  [19.49373927  0.28892871]\n"]}],"source":["mean = mean.reshape(2)\n","std = std.reshape(2)\n","print(\"Mean: \", mean)\n","print(\"Std: \", std)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mqlYFHxDPNY"},"outputs":[],"source":["print(\"Train size: \", len(train_data))\n","print(\"Valid size: \", len(valid_data))\n","print(\"Test size: \", len(test_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1Q3vI7hdo9q"},"outputs":[],"source":["lap_pos = calculate_lap_pos(g, 100)\n","lap_pos = torch.tensor(lap_pos).float()\n","g.ndata['lap_pos'] = lap_pos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSLKMsLyCHZg"},"outputs":[],"source":["batch_size = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"984oJ9oM4_Dy"},"outputs":[],"source":["train_loader = DataLoader(train_data, batch_size= batch_size, num_workers= 2, shuffle=True)\n","valid_loader = DataLoader(valid_data, batch_size= batch_size, num_workers= 2, shuffle=False)\n","test_loader  = DataLoader(test_data,  batch_size= batch_size, num_workers= 2, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1661869740582,"user":{"displayName":"phú nguyễn","userId":"01399564067477344093"},"user_tz":-420},"id":"J9WeHBTi55ge","outputId":"bfd41b0f-3d2d-4716-afb6-128d1ba858a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gptODY8tYEIQ"},"outputs":[],"source":["def masked_mae_loss(y_pred, y_true):\n","    mask = (y_true != 0).float()\n","    mask /= mask.mean()\n","    loss = torch.abs(y_pred - y_true)\n","    loss = loss * mask\n","    # trick for nans: https://discuss.pytorch.org/t/how-to-set-nan-in-tensor-to-0/3918/3\n","    loss[loss != loss] = 0\n","    return loss.mean()\n","\n","def masked_mse_loss(y_pred, y_true):\n","    mask = (y_true != 0).float()\n","    mask /= mask.mean()\n","    loss = (y_pred- y_true)**2\n","    loss = loss * mask\n","    # trick for nans: https://discuss.pytorch.org/t/how-to-set-nan-in-tensor-to-0/3918/3\n","    loss[loss != loss] = 0\n","    return loss.mean()\n","\n","def masked_mape_loss(y_pred, y_true):\n","    mask = (y_true != 0).float()\n","    mask /= mask.mean()\n","    loss = (y_true - y_pred).abs() / y_true.abs()\n","    loss = loss * mask\n","    # trick for nans: https://discuss.pytorch.org/t/how-to-set-nan-in-tensor-to-0/3918/3\n","    loss[loss != loss] = 0\n","    return loss.mean()"]},{"cell_type":"markdown","metadata":{"id":"v7tN_YUjfNpP"},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4384,"status":"ok","timestamp":1661869744949,"user":{"displayName":"phú nguyễn","userId":"01399564067477344093"},"user_tz":-420},"id":"convszPy4_Dz","outputId":"9c4166d1-b903-4bf4-ace2-c6b724742211"},"outputs":[{"output_type":"stream","name":"stdout","text":["Start\n","Trainable parameters: 80016\n"]}],"source":["num_decode_steps = seq_len = 12\n","num_encode_steps = 12\n","model = None\n","model = STGPCN(n_filter=num_encode_steps,\n","               in_channels = 2, time_strides = 1, num_for_predict = num_decode_steps, len_input = num_encode_steps\n","               ).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3, amsgrad = True)\n","loss_fn = masked_mae_loss\n","print('Trainable parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"625rXIMUf5X_"},"outputs":[],"source":["# Run this cell when you already have a checkpoint\n","#checkpoint = torch.load('model_stt_week_day_sample.pt')\n","#model.load_state_dict(checkpoint['model_state_dict'])\n","#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"markdown","metadata":{"id":"0uXvLVNFFX2k"},"source":["## Train "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BY2sKNYDoXCg"},"outputs":[],"source":["# Run this cell to load the model that is saved during training\n","checkpoint = torch.load('best/model_metrla_stgpcn_window.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-QgCaEhGByr"},"outputs":[],"source":["best_loss = [100]\n","try:\n","    best_loss[0] = checkpoint['loss']\n","    epoch = checkpoint['epoch']\n","except:\n","    best_loss[0] = 100\n","    epoch = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSG1WpcVNY-E"},"outputs":[],"source":["epoch = 0\n","best_loss = [100]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9JLoEXSDILS"},"outputs":[],"source":["print(\"Best Loss: \", best_loss[0])\n","print(\"Epoch: \", epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMsNped0-AtE"},"outputs":[],"source":["import numpy as np\n","def train_1(model, num_node, g, train_loader, lap_pos, scaler, loss_fn, optimizer, batch_size, num_samples, seq_len):\n","    total_loss = []\n","    for idx, (x, y) in tqdm(enumerate(train_loader), total = num_samples // batch_size):\n","        '''\n","        lap_pos = g.ndata['lap_pos']\n","        sign_flip = torch.rand(lap_pos.size(1))\n","        sign_flip[sign_flip>=0.5] = 1.0; sign_flip[sign_flip<0.5] = -1.0\n","        lap_pos = lap_pos * sign_flip.unsqueeze(0)\n","        g.ndata['lap_pos'] = lap_pos\n","        '''\n","        \n","        bs, _, num_node, dim = x.shape\n","\n","        batch_g = dgl.batch([g] * bs).to(device)\n","        y = y[..., 0]\n","        # x = x.permute(1, 0, 2, 3)\n","        # y = y.permute(1, 0, 2)\n","        x = x.permute(0, 2, 3, 1)[:, :, 0, :].unsqueeze(2)\n","        y = y.permute(0, 2, 1)[:, :, :seq_len]\n","\n","        x = scaler.scale(x, mean[0],std[0])\n","        x = torch.tensor(x).float().clone().to(device)\n","        #y = torch.tensor(y).float().clone().to(device)\n","\n","        targets = y[:-1]\n","        targets = scaler.scale(targets, mean[0],std[0])\n","        targets = torch.tensor(targets).float().clone().to(device)\n","        y = torch.tensor(y).float().clone().to(device)\n","\n","        out = model(x).squeeze()\n","        #out = scaler.inverse_scale(out, mean[0], std[0])\n","        out = out * std[0] + mean[0]\n","        loss = loss_fn(out, y)\n","        loss.backward()\n","        #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        \n","        total_loss.append(loss.detach().item())\n","    return np.mean(total_loss)\n","   \n","\n","def eval_1(model, num_node, g, val_loader, lap_pos, scaler, loss_fn, batch_size, num_samples, seq_len):\n","    model.eval()\n","    total_loss = []\n","    for idx, (x, y) in tqdm(enumerate(val_loader), total = num_samples // batch_size):\n","        '''\n","        lap_pos = g.ndata['lap_pos']\n","        sign_flip = torch.rand(lap_pos.size(1))\n","        sign_flip[sign_flip>=0.5] = 1.0; sign_flip[sign_flip<0.5] = -1.0\n","        lap_pos = lap_pos * sign_flip.unsqueeze(0)\n","        g.ndata['lap_pos'] = lap_pos\n","        '''\n","        \n","        bs, _, num_node, dim = x.shape\n","\n","        batch_g = dgl.batch([g] * bs).to(device)\n","\n","        y = y[..., 0]\n","        # x = x.permute(1, 0, 2, 3)\n","        # y = y.permute(1, 0, 2)\n","        x = x.permute(0, 2, 3, 1)[:, :, 0, :].unsqueeze(2)\n","        y = y.permute(0, 2, 1)[:, :, :seq_len]\n","\n","        x = scaler.scale(x, mean[0],std[0])\n","        x = torch.tensor(x).float().clone().to(device)\n","        #y = torch.tensor(y).float().clone().to(device)\n","\n","        targets = y[:-1]\n","        targets = scaler.scale(targets, mean[0],std[0])\n","        targets = torch.tensor(targets).float().clone().to(device)\n","        y = torch.tensor(y).float().clone().to(device)\n","  \n","        with torch.no_grad():\n","            out = model(x).squeeze()\n","        #out = scaler.inverse_scale(out, mean[0], std[0])\n","        out = out * std[0] + mean[0]\n","        loss = loss_fn(out, y)\n","        #loss = torch.sqrt(loss_fn(out, y))\n","        total_loss.append(loss.detach().item())\n","        del loss\n","\n","    return np.mean(total_loss)\n","\n","def test_1(model, num_node, g, val_loader, lap_pos, scaler, metrics, batch_size, num_samples, num_steps, seq_len):\n","    model.eval()\n","    total_rmse = []\n","    total_mae = []\n","    total_mape = []\n","    predictions = []\n","    actuals = []\n","    for idx, (x, y) in tqdm(enumerate(val_loader), total = num_samples // batch_size):\n","\n","        '''\n","        g.ndata['lap_pos'] = lap_pos\n","        '''\n","        \n","        bs, _ , num_node, dim = x.shape\n","        \n","        batch_g = dgl.batch([g] * bs).to(device)\n","\n","        y = y[..., 0]\n","        # x = x.permute(1, 0, 2, 3)\n","        # y = y.permute(1, 0, 2)\n","        print(y.shape)\n","        x = x.permute(0, 2, 3, 1)[:, :, 0, :].unsqueeze(2)\n","        y = y.permute(0, 2, 1)[:, :, :seq_len]\n","\n","        x = scaler.scale(x, mean[0],std[0])\n","        x = torch.tensor(x).float().clone().to(device)\n","        #y = torch.tensor(y).float().clone().to(device)\n","\n","        targets = y[:-1]\n","        targets = scaler.scale(targets, mean[0],std[0])\n","        targets = torch.tensor(targets).float().clone().to(device)\n","        y = torch.tensor(y).float().clone().to(device)\n","\n","        with torch.no_grad():\n","            out = model(x).squeeze()\n","        #out = scaler.inverse_scale(out, mean[ 0], std[0])\n","        out = out * std[0] + mean[0]\n","\n","        out = out\n","        y = y\n","        mae = metrics['mae'](out, y).detach().item()\n","        rmse = torch.sqrt(metrics['mse'](out, y)).detach().item()\n","        mape = metrics['mape'](out, y).detach().item()\n","\n","        total_mae.append(mae)\n","        total_rmse.append(rmse)\n","        total_mape.append(mape)\n","        \n","        out = out.detach().cpu().numpy()\n","        y = y.detach().cpu().numpy()\n","\n","        out = np.transpose(out, axes = (1, 2, 0))\n","        y = np.transpose(y, axes = (1, 2, 0))\n","\n","        predictions += out[:, -1, 0].tolist()\n","        actuals += y[:, -1, 0].tolist()\n","\n","    return np.mean(total_mae), np.mean(total_rmse), np.mean(total_mape), predictions, actuals\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyPvL4_j_Exf"},"outputs":[],"source":["# print(\"Best Loss\", best_loss[0])\n","for e in range(1, 31):\n","    print(\"Epoch: \", epoch + e)\n","    print(\"Best Validation Loss\", best_loss[0])\n","    tmp_loss = best_loss[0]\n","    # loss_fn = nn.L1Loss() # We should use L1Loss for STREETS because there is no nan and 0 is meaningful\n","    train_loss = train_1(model, num_node, g, train_loader, lap_pos, scaler, loss_fn, optimizer, batch_size, len(train_data), seq_len)\n","    val_loss = eval_1(model, num_node, g, valid_loader, lap_pos, scaler, loss_fn, batch_size, len(valid_data), seq_len)\n","    torch.save({\n","            'epoch' : epoch + e, \n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss' : val_loss,\n","            }, 'weight/model_metrla_stgpcn_window.pt')\n","    #print(\"Train Loss: {}\".format(train_loss))\n","    print(\"Train Loss: {}, Val Loss: {}\".format(train_loss, val_loss))\n","    \n","    if best_loss[0] > val_loss:\n","        best_loss[0] = val_loss\n","        torch.save({\n","                'epoch' : epoch + e, \n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss' : val_loss,\n","                }, 'best/model_metrla_stgpcn_window.pt')\n","        print(\"------------- Saved model ------------\")\n","    print()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byUAbtCV4a-V"},"outputs":[],"source":["for x in checkpoint:\n","  print(x)"]},{"cell_type":"markdown","metadata":{"id":"pFrbKTMQVhsG"},"source":["## Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85q-9KGrYOk_"},"outputs":[],"source":["# Run this cell to load the model that is saved during training\n","checkpoint = torch.load('best/model_metrla_stgpcn_window.pt')\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","try:\n","    best_loss[0] = checkpoint['loss']\n","    epoch = checkpoint['epoch']\n","except:\n","    best_loss[0] = 100\n","    epoch = 100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9C2Cjz1b1t4"},"outputs":[],"source":["print(best_loss[0])\n","print(epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pruZ8XIHTV5Q"},"outputs":[],"source":["metrics = {\n","    'mae' : masked_mae_loss,\n","    'mse' : masked_mse_loss,\n","    'mape': masked_mape_loss\n","}\n","mae_loss, rmse_loss, mape_loss, predictions, gts = test_1(model, num_node, g, test_loader, lap_pos, scaler, metrics, batch_size, len(test_data), 12)\n","print(\"MAE: {}\".format(mae_loss))\n","print(\"RMSE: {}\".format(rmse_loss))\n","print(\"MAPE: {}\".format(mape_loss))"]},{"cell_type":"markdown","metadata":{"id":"PSj8O-9bFdiJ"},"source":["## Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dac6oyTC4_D2"},"outputs":[],"source":["print(predictions[10])\n","print(gts[10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8gbKmRw4_D3"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6xRgKC9qKbM"},"outputs":[],"source":["def visualize(preds, gts, start, end, data):\n","    horizon = end - start\n","    x = np.arange(horizon)\n","    plt.plot(x, preds[start : end], label = 'Prediction')\n","    plt.plot(x, gts[start : end], label = 'Ground Truth')\n","    plt.legend(loc = \"lower center\")\n","    plt.savefig(f\"report/{data}_{start}_{end}.png\")\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cCkwXUKu4_D4"},"outputs":[],"source":["predictions = np.array(predictions)\n","gts = np.array(gts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfmOpaLtrf0l"},"outputs":[],"source":["visualize(predictions, gts, 1200, 1300, 'metr-la')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EBcMbHsdbGFn"},"outputs":[],"source":["import random\n","for i in range(10):\n","    idx = random.randint(0, len(gts))\n","    print(\"Actual: \", gts[idx])\n","    print(\"Prediction: \", predictions[idx])\n","    print()\n","    print()    "]},{"cell_type":"markdown","metadata":{"id":"L3Z8mFhCA9JF"},"source":["# STREETS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMPiQ558DIYU"},"outputs":[],"source":["# This box is for STREETS dataset\n","\n","testing_dates = ['2019-7-15', '2019-7-16', '2019-7-17', '2019-7-18']\n","test_loaders = {}\n","\n","for date in testing_dates:\n","    # test_data = BuffalogroveTestDataset(date, num_steps = 6)\n","    test_data = GurneeTestDataset(date, num_steps = 3)\n","    print(len(test_data))\n","    test_loader  = DataLoader(test_data,  batch_size= 16, num_workers= 2, shuffle=False)\n","    print(len(test_loader))\n","    test_loaders[date] = test_loader\n","metrics = {\n","    'mae' : masked_mae_loss,\n","    'mse' : masked_mse_loss,\n","    'mape': masked_mape_loss\n","}\n","results = {}\n","gts = {}\n","preds = {}\n","for date in testing_dates:\n","    test_loader = test_loaders[date]\n","    val_loss = eval_1(model, num_node, g, test_loader, lap_pos, scaler, loss_fn, batch_size, len(test_data), seq_len)\n","    results[date] = val_loss\n","for date in testing_dates:\n","    print(results[date])\n","    print()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"STGPCN.ipynb","provenance":[{"file_id":"1GjD3-U-eB40RkDb9SnTXm47HRuzlAdDM","timestamp":1660587272589},{"file_id":"1gfOi8cvK8InLg-xqM0eS133tSSnvCq9t","timestamp":1651741440133}]},"gpuClass":"standard","interpreter":{"hash":"59636f6344ea3cd51dac5e7e986efbbd8a2475562bac4dfc3265cc50c0e0e332"},"kernelspec":{"display_name":"Python 3.8.12 ('dgl')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}